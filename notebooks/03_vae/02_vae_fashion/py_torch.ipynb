{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸ‘– Variational Autoencoders - Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own autoencoder on the fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8071b-6381-4147-8755-ea2ee48e8ab3",
   "metadata": {},
   "source": [
    "The code has been adapted from the excellent [VAE tutorial](https://keras.io/examples/generative/vae/) created by Francois Chollet, available on the Keras website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 2\n",
    "EPOCHS = 5\n",
    "BETA = 500\n",
    "LEARNING_RATE = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int((1 - VALIDATION_SPLIT) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53709f-7f3f-483b-9db8-2e5f9b9942c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some items of clothing from the training set\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(f\"Label: {example_targets[i]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  }
  {
    "cell_type": "markdown",
    "id": "35b14665-4359-447b-be58-3fd58ba69084",
    "metadata": {},
    "source": [
     "## 3. Train the variational autoencoder <a name=\"train\"></a>"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "b429fdad-ea9c-45a2-a556-eb950d793824",
    "metadata": {},
    "outputs": [],
    "source": [
     "class VAE(nn.Module):\n",
     "    def __init__(self, encoder, decoder):\n",
     "        super(VAE, self).__init__()\n",
     "        self.encoder = encoder\n",
     "        self.decoder = decoder\n",
     "\n",
     "    def forward(self, x):\n",
     "        z_mean, z_log_var, z = self.encoder(x)\n",
     "        reconstruction = self.decoder(z)\n",
     "        return reconstruction, z_mean, z_log_var\n",
     "\n",
     "def vae_loss(recon_x, x, mu, logvar, beta):\n",
     "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
     "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
     "    return BCE + beta * KLD\n",
     "\n",
     "vae = VAE(encoder, decoder)\n",
     "optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
     "\n",
     "def train(epochs):\n",
     "    vae.train()\n",
     "    for epoch in range(epochs):\n",
     "        train_loss = 0\n",
     "        for batch_idx, (data, _) in enumerate(train_loader):\n",
     "            data = data.view(-1, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
     "            optimizer.zero_grad()\n",
     "            recon_batch, mu, logvar = vae(data)\n",
     "            loss = vae_loss(recon_batch, data, mu, logvar, BETA)\n",
     "            loss.backward()\n",
     "            train_loss += loss.item()\n",
     "            optimizer.step()\n",
     "\n",
     "            if batch_idx % 100 == 0:\n",
     "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item() / len(data):.6f}')\n",
     "        print(f'====> Epoch: {epoch+1}, Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
     "\n",
     "train(EPOCHS)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "07ab76a1-c4b8-44e3-9c0f-00622fa82277",
    "metadata": {},
    "source": [
     "## 4. Embed using the encoder <a name=\"encode\"></a>"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "1aac4bca-15fa-4679-b790-1347854c22b0",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Encode the example images\n",
     "vae.eval()\n",
     "with torch.no_grad():\n",
     "    example_images, _ = next(iter(test_loader))\n",
     "    example_images = example_images.view(-1, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
     "    z_mean, z_log_var, z = vae.encoder(example_images)\n",
     "    z = z.numpy()\n",
     "\n",
     "# Some examples of the embeddings\n",
     "print(z[:10])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "02736f3a-2446-4e4c-a8c1-f1eb34729f03",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Show the encoded points in 2D space\n",
     "figsize = 8\n",
     "\n",
     "plt.figure(figsize=(figsize, figsize))\n",
     "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=3)\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "f8fb22e1-f73f-4b97-8a8a-787c1c5e605a",
    "metadata": {},
    "source": [
     "## 5. Generate using the decoder <a name=\"decode\"></a>"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "c8cbb2e9-4ba6-4332-b5cf-c509472b4d39",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Sample some points in the latent space, from the standard normal distribution\n",
     "grid_width, grid_height = (6, 3)\n",
     "z_sample = np.random.normal(size=(grid_width * grid_height, EMBEDDING_DIM))\n",
     "z_sample = torch.from_numpy(z_sample).float()\n",
     "\n",
     "# Decode the sampled points\n",
     "vae.eval()\n",
     "with torch.no_grad():\n",
     "    reconstructions = vae.decoder(z_sample).numpy()\n",
     "\n",
     "# Draw a plot of...\n",
     "figsize = 8\n",
     "plt.figure(figsize=(figsize, figsize))\n",
     "\n",
     "# ... the original embeddings ...\n",
     "plt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n",
     "\n",
     "# ... and the newly generated points in the latent space\n",
     "plt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\n",
     "plt.show()\n",
     "\n",
     "# Add underneath a grid of the decoded images\n",
     "fig = plt.figure(figsize=(figsize, grid_height * 2))\n",
     "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
     "\n",
     "for i in range(grid_width * grid_height):\n",
     "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
     "    ax.axis(\"off\")\n",
     "    ax.text(\n",
     "        0.5,\n",
     "        -0.35,\n",
     "        str(np.round(z_sample[i, :].numpy(), 1)),\n",
     "        fontsize=10,\n",
     "        ha=\"center\",\n",
     "        transform=ax.transAxes,\n",
     "    )\n",
     "    ax.imshow(reconstructions[i, 0, :, :], cmap=\"Greys\")"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a70a79b8-b109-4a47-8364-f0edc15a0e7f",
    "metadata": {},
    "source": [
     "## 6. Explore the latent space <a name=\"explore\"></a>"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "9547247c-0e2a-4ac5-9490-4a1346a3f527",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Colour the embeddings by their label (clothing type - see table)\n",
     "figsize = 8\n",
     "fig = plt.figure(figsize=(figsize * 2, figsize))\n",
     "ax = fig.add_subplot(1, 2, 1)\n",
     "plot_1 = ax.scatter(\n",
     "    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n",
     ")\n",
     "plt.colorbar(plot_1)\n",
     "ax = fig.add_subplot(1, 2, 2)\n",
     "plot_2 = ax.scatter(\n",
     "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n",
     ")\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "4e176e97-8ee6-491a-9f93-92e9e526a2c7",
    "metadata": {},
    "source": [
     "| ID | Clothing Label |\n",
     "| :- | :- |\n",
     "| 0 | T-shirt/top |\n",
     "| 1 | Trouser |\n",
     "| 2 | Pullover |\n",
     "| 3 | Dress |\n",
     "| 4 | Coat |\n",
     "| 5 | Sandal |\n",
     "| 6 | Shirt |\n",
     "| 7 | Sneaker |\n",
     "| 8 | Bag |\n",
     "| 9 | Ankle boot |"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "276fdfa2-b5e9-41fe-b13d-fde00e3edff4",
    "metadata": {},
    "outputs": [],
    "source": [
     "# Colour the embeddings by their label (clothing type - see table)\n",
     "figsize = 12\n",
     "grid_size = 15\n",
     "plt.figure(figsize=(figsize, figsize))\n",
     "plt.scatter(\n",
     "    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n",
     ")\n",
     "plt.colorbar()\n",
     "\n",
     "x = norm.ppf(np.linspace(0, 1, grid_size))\n",
     "y = norm.ppf(np.linspace(1, 0, grid_size))\n",
     "xv, yv = np.meshgrid(x, y)\n",
     "xv = xv.flatten()\n",
     "yv = yv.flatten()\n",
     "grid = np.array(list(zip(xv, yv)))\n",
     "\n",
     "reconstructions = vae.decoder(torch.from_numpy(grid).float()).detach().numpy()\n",
     "# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\n",
     "plt.show()\n",
     "\n",
     "fig = plt.figure(figsize=(figsize, figsize))\n",
     "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
     "for i in range(grid_size**2):\n",
     "    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
     "    ax.axis(\"off\")\n",
     "    ax.imshow(reconstructions[i, 0, :, :], cmap=\"Greys\")"
    ]
   }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },

   "nbformat": 4,
   "nbformat_minor": 5
  }